{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:33:12.906566Z","iopub.status.busy":"2025-03-29T05:33:12.906170Z","iopub.status.idle":"2025-03-29T05:33:12.909968Z","shell.execute_reply":"2025-03-29T05:33:12.909198Z","shell.execute_reply.started":"2025-03-29T05:33:12.906535Z"},"trusted":true},"outputs":[],"source":["# # Cài đặt các gói cần thiết\n","# !pip install pytorch-lightning\n","# !pip install torchmetrics\n","# !pip install transformers\n","# !pip install datasets"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-03-31T09:23:13.457121Z","iopub.status.busy":"2025-03-31T09:23:13.456823Z","iopub.status.idle":"2025-03-31T09:23:35.753263Z","shell.execute_reply":"2025-03-31T09:23:35.752335Z","shell.execute_reply.started":"2025-03-31T09:23:13.457088Z"},"trusted":true},"outputs":[],"source":["# Import các thư viện cần thiết\n","import os\n","import zipfile\n","from pathlib import Path\n","from typing import Any, Dict, List, Optional, Tuple, Union\n","from urllib.request import urlretrieve\n","\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import pytorch_lightning as pl\n","import torch\n","import torch.nn.functional as F\n","import torchmetrics\n","from datasets import load_dataset\n","from pytorch_lightning import loggers as pl_loggers\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from torch.utils.data import DataLoader\n","from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n","                          DataCollatorWithPadding)\n","\n","# Đặt seed để đảm bảo tính tái tạo\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:33:34.079057Z","iopub.status.busy":"2025-03-29T05:33:34.078389Z","iopub.status.idle":"2025-03-29T05:33:34.088155Z","shell.execute_reply":"2025-03-29T05:33:34.087331Z","shell.execute_reply.started":"2025-03-29T05:33:34.079022Z"},"trusted":true},"outputs":[],"source":["class TqdmUpTo(tqdm):\n","    \"\"\"From https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py\"\"\"\n","\n","    def update_to(self, blocks=1, bsize=1, tsize=None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        blocks: int, optional\n","            Number of blocks transferred so far [default: 1].\n","        bsize: int, optional\n","            Size of each block (in tqdm units) [default: 1].\n","        tsize: int, optional\n","            Total size (in tqdm units). If [default: None] remains unchanged.\n","        \"\"\"\n","        if tsize is not None:\n","            self.total = tsize  # pylint: disable=attribute-defined-outside-init\n","        self.update(blocks * bsize - self.n)  # will also set self.n = b * bsize\n","\n","\n","def download_url(url, filename, directory='.'):\n","    \"\"\"Download a file from url to filename, with a progress bar.\"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","    path = os.path.join(directory, filename)\n","\n","    with TqdmUpTo(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1) as t:\n","        urlretrieve(url, path, reporthook=t.update_to, data=None)  # nosec\n","    return path\n","\n","def _load_data_from(data_dir: Union[str, Path]):\n","    \"\"\"Load dữ liệu cảm xúc từ tệp văn bản\"\"\"\n","    fnames = ['sentiments.txt', 'sents.txt', 'topics.txt']\n","    sentiments = []\n","    sents = []\n","    topics = []\n","    for name in fnames:\n","        with open(f\"{data_dir}/{name}\", 'r') as f:\n","            if name == \"sentiments.txt\":\n","                sentiments = [int(line.strip()) for line in f.readlines()]\n","            elif name == \"sents.txt\":\n","                sents = [line.strip() for line in f.readlines()]        \n","            else:\n","                topics = [int(line.strip()) for line in f.readlines()]\n","    return sents, sentiments, topics\n","\n","def _save_to_csv(file_path: Union[str, Path], data):\n","    \"\"\"Chuyển đổi dữ liệu sang định dạng CSV\"\"\"\n","    sents, sentiments, topics = data\n","    df = pd.DataFrame({\n","        \"sents\": sents,\n","        \"labels\": sentiments,\n","        \"topics\": topics\n","    })\n","    df.to_csv(file_path, index=False)\n","    return file_path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:33:34.090109Z","iopub.status.busy":"2025-03-29T05:33:34.089867Z","iopub.status.idle":"2025-03-29T05:33:34.121396Z","shell.execute_reply":"2025-03-29T05:33:34.120795Z","shell.execute_reply.started":"2025-03-29T05:33:34.090090Z"},"trusted":true},"outputs":[],"source":["class UIT_VSFC(pl.LightningDataModule):\n","    \"\"\"\n","    UIT-VSFC: Vietnamese Students' Feedback Corpus for sentiment analysis\n","    \"\"\"\n","    def __init__(self, tokenizer, opts: Dict[str, Any]):\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.batch_size = opts['batch_size']\n","        self.num_workers = opts['num_workers']\n","        self.on_gpu = opts['on_gpu']\n","        self.data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","        self.dataset = None\n","        self.mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n","        self.inverse_mapping = {v: k for k, v in self.mapping.items()}\n","        \n","        # Tải dataset luôn khi khởi tạo để tránh lỗi setup\n","        self.raw_datasets = load_dataset(\"uitnlp/vietnamese_students_feedback\")\n","        \n","    def prepare_data(self):\n","        # Phương thức này chỉ gọi một lần và không cần trả về gì\n","        pass\n","\n","    def setup(self, stage=None):\n","        # Kiểm tra cấu trúc dữ liệu\n","        sample = self.raw_datasets['train'][0]\n","        print(\"Cấu trúc dữ liệu mẫu:\", sample)\n","        \n","        # Hàm tokenize đầu vào\n","        def tokenize_function(examples):\n","            # Thêm truncation=True để đảm bảo độ dài phù hợp\n","            return self.tokenizer(examples['sentence'], truncation=True, max_length=256)\n","        \n","        if self.dataset is None:\n","            # Tạo dataset nếu chưa được tạo\n","            splits = {\n","                'train': self.raw_datasets['train'],\n","                'dev': self.raw_datasets['validation'],\n","                'test': self.raw_datasets['test']\n","            }\n","            \n","            # Áp dụng tokenize và định dạng\n","            self.dataset = {}\n","            for split_name, split_data in splits.items():\n","                print(f\"Xử lý split: {split_name}\")\n","                # Áp dụng tokenizer\n","                try:\n","                    # Áp dụng tokenizer\n","                    tokenized_data = split_data.map(\n","                        tokenize_function,\n","                        batched=True,\n","                        remove_columns=['sentence', 'topic']  # Xóa cột không cần thiết\n","                    )\n","                    \n","                    # In thông tin cột để debug\n","                    print(f\"Các cột sau khi tokenize: {tokenized_data.column_names}\")\n","                    \n","                    # Lưu lại dữ liệu tokenized trước khi chuyển đổi định dạng\n","                    self.dataset[split_name] = tokenized_data\n","                    \n","                    # Tạo dataset với các trường cần thiết\n","                    formatted_data = tokenized_data.with_format(\n","                        type='torch',\n","                        columns=['input_ids', 'attention_mask', 'sentiment'],\n","                        output_all_columns=False  # Chỉ giữ lại các cột đã chỉ định\n","                    )\n","                    \n","                    # Kiểm tra dữ liệu đã được định dạng\n","                    if formatted_data is not None:\n","                        # Đổi tên cột\n","                        formatted_data = formatted_data.rename_column('sentiment', 'labels')\n","                        self.dataset[split_name] = formatted_data\n","                    else:\n","                        print(f\"Lỗi: Không thể định dạng dữ liệu cho split {split_name}\")\n","                \n","                except Exception as e:\n","                    print(f\"Lỗi khi xử lý split {split_name}: {str(e)}\")\n","                    # Sử dụng cách tiếp cận thủ công nếu cách trên không hoạt động\n","                    try:\n","                        print(\"Thử phương pháp thay thế...\")\n","                        # Tiếp cận thủ công để tạo tensor\n","                        tokenized_data = split_data.map(\n","                            tokenize_function,\n","                            batched=True\n","                        )\n","                        \n","                        # Chuyển đổi thành dict of lists\n","                        data_dict = {\n","                            'input_ids': tokenized_data['input_ids'],\n","                            'attention_mask': tokenized_data['attention_mask'],\n","                            'labels': tokenized_data['sentiment']  # Đổi tên ngay tại đây\n","                        }\n","                        \n","                        # Tạo datasets từ dict\n","                        from datasets import Dataset\n","                        self.dataset[split_name] = Dataset.from_dict(data_dict).with_format(\"torch\")\n","                    except Exception as e2:\n","                        print(f\"Cả hai phương pháp đều thất bại cho split {split_name}: {str(e2)}\")\n","                        continue\n","    \n","    def train_dataloader(self):\n","        # Đảm bảo setup đã được gọi\n","        if self.dataset is None:\n","            self.setup()\n","            \n","        if 'train' not in self.dataset or self.dataset['train'] is None:\n","            raise ValueError(\"Train dataset không khả dụng\")\n","            \n","        return DataLoader(\n","            self.dataset['train'],\n","            shuffle=True,\n","            batch_size=self.batch_size,\n","            num_workers=self.num_workers,\n","            pin_memory=self.on_gpu,\n","            collate_fn=self.data_collator\n","        )\n","    \n","    def val_dataloader(self):\n","        # Đảm bảo setup đã được gọi\n","        if self.dataset is None:\n","            self.setup()\n","            \n","        if 'dev' not in self.dataset or self.dataset['dev'] is None:\n","            raise ValueError(\"Validation dataset không khả dụng\")\n","            \n","        return DataLoader(\n","            self.dataset['dev'],\n","            shuffle=False,\n","            batch_size=self.batch_size,\n","            num_workers=self.num_workers,\n","            pin_memory=self.on_gpu,\n","            collate_fn=self.data_collator\n","        )\n","    \n","    def test_dataloader(self):\n","        # Đảm bảo setup đã được gọi\n","        if self.dataset is None:\n","            self.setup()\n","            \n","        if 'test' not in self.dataset or self.dataset['test'] is None:\n","            raise ValueError(\"Test dataset không khả dụng\")\n","            \n","        return DataLoader(\n","            self.dataset['test'],\n","            shuffle=False,\n","            batch_size=self.batch_size,\n","            num_workers=self.num_workers,\n","            pin_memory=self.on_gpu,\n","            collate_fn=self.data_collator\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:33:34.122650Z","iopub.status.busy":"2025-03-29T05:33:34.122413Z","iopub.status.idle":"2025-03-29T05:33:34.143360Z","shell.execute_reply":"2025-03-29T05:33:34.142779Z","shell.execute_reply.started":"2025-03-29T05:33:34.122630Z"},"trusted":true},"outputs":[],"source":["class PhoBERT(pl.LightningModule):\n","    def __init__(self, lr, weight_decay):\n","        super().__init__()\n","        self.model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n","        self.lr = lr\n","        self.weight_decay = weight_decay\n","\n","        # Định nghĩa các metrics\n","        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n","        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n","        self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=3)\n","        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n","        self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=3)\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n","    \n","    def training_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        loss, logits = outputs.loss, outputs.logits\n","        sentiments = batch['labels']\n","        scores = F.softmax(logits, dim=-1)\n","        self.train_acc(scores, sentiments)\n","        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        loss, logits = outputs.loss, outputs.logits\n","        sentiments = batch['labels']\n","        scores = F.softmax(logits, dim=-1)\n","        self.val_acc(scores, sentiments)\n","        self.val_f1(scores, sentiments)\n","        \n","        # Cải thiện cách log metrics\n","        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n","\n","    def test_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        logits = outputs.logits\n","        sentiments = batch['labels']\n","        scores = F.softmax(logits, dim=-1)\n","        self.test_acc(scores, sentiments)\n","        self.test_f1(scores, sentiments)\n","        \n","        # Cải thiện cách log metrics\n","        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True, logger=True)\n","        self.log('test_f1', self.test_f1, on_step=False, on_epoch=True, logger=True)\n","\n","    # Thêm phương thức on_validation_epoch_end để đảm bảo metrics được tính toán đầy đủ\n","    def on_validation_epoch_end(self):\n","        # Log lại metrics một lần nữa ở cuối epoch\n","        self.log('val_acc_epoch', self.val_acc.compute(), prog_bar=True, logger=True)\n","        self.log('val_f1_epoch', self.val_f1.compute(), prog_bar=True, logger=True)\n","        \n","        # In thông tin để debug\n","        print(f\"Epoch end val_f1: {self.val_f1.compute():.4f}, val_acc: {self.val_acc.compute():.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:33:34.144403Z","iopub.status.busy":"2025-03-29T05:33:34.144178Z","iopub.status.idle":"2025-03-29T05:33:34.160600Z","shell.execute_reply":"2025-03-29T05:33:34.159832Z","shell.execute_reply.started":"2025-03-29T05:33:34.144385Z"},"trusted":true},"outputs":[],"source":["    def validation_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        loss, logits = outputs.loss, outputs.logits\n","        sentiments = batch['labels']\n","        scores = F.softmax(logits, dim=-1)\n","        self.val_acc(scores, sentiments)\n","        self.val_f1(scores, sentiments)\n","        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","    def test_step(self, batch, batch_idx):\n","        outputs = self.model(**batch)\n","        logits = outputs.logits\n","        sentiments = batch['labels']\n","        scores = F.softmax(logits, dim=-1)\n","        self.test_acc(scores, sentiments)\n","        self.test_f1(scores, sentiments)\n","        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True, logger=True)\n","        self.log('test_f1', self.test_f1, on_step=False, on_epoch=True, logger=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:33:34.161728Z","iopub.status.busy":"2025-03-29T05:33:34.161451Z","iopub.status.idle":"2025-03-29T05:46:11.116959Z","shell.execute_reply":"2025-03-29T05:46:11.116191Z","shell.execute_reply.started":"2025-03-29T05:33:34.161702Z"},"trusted":true},"outputs":[],"source":["# Khởi tạo tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","\n","# Tùy chọn cho datamodule\n","options = {\n","    \"on_gpu\": torch.cuda.is_available(),\n","    \"batch_size\": 16,\n","    \"num_workers\": 2\n","}\n","\n","# Khởi tạo datamodule\n","datamodule = UIT_VSFC(tokenizer, options)\n","\n","# Các siêu tham số\n","lr = 2e-5\n","weight_decay = 0.01\n","\n","# Khởi tạo mô hình\n","model = PhoBERT(lr, weight_decay)\n","\n","# Thiết lập callback lưu checkpoint\n","checkpoint_callback = ModelCheckpoint(\n","    monitor='val_f1',\n","    dirpath='checkpoints',\n","    filename='phobert-sentiment-{epoch:02d}-{val_f1:.4f}',\n","    save_top_k=1,\n","    mode='max',\n",")\n","\n","# Khởi tạo trainer\n","trainer = pl.Trainer(\n","    max_epochs=10,\n","    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n","    devices=1,\n","    callbacks=[checkpoint_callback],\n","    deterministic=True,\n",")\n","\n","# Kiểm tra dataloader trước khi huấn luyện (tùy chọn)\n","datamodule.setup()\n","batch = next(iter(datamodule.train_dataloader()))\n","print(\"Batch keys:\", batch.keys())\n","print(\"Input ids shape:\", batch['input_ids'].shape)\n","print(\"Labels shape:\", batch['labels'].shape if 'labels' in batch else \"Labels not found\")\n","\n","# Huấn luyện mô hình\n","trainer.fit(model, datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:46:11.118308Z","iopub.status.busy":"2025-03-29T05:46:11.117962Z","iopub.status.idle":"2025-03-29T05:46:18.853929Z","shell.execute_reply":"2025-03-29T05:46:18.852916Z","shell.execute_reply.started":"2025-03-29T05:46:11.118273Z"},"trusted":true},"outputs":[],"source":["# Kiểm tra mô hình sử dụng checkpoint tốt nhất\n","test_results = trainer.test(ckpt_path=checkpoint_callback.best_model_path, datamodule=datamodule)\n","print(f\"Kết quả kiểm tra: {test_results}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:46:18.856219Z","iopub.status.busy":"2025-03-29T05:46:18.855977Z","iopub.status.idle":"2025-03-29T05:46:19.141522Z","shell.execute_reply":"2025-03-29T05:46:19.140632Z","shell.execute_reply.started":"2025-03-29T05:46:18.856196Z"},"trusted":true},"outputs":[],"source":["# Hiển thị kết quả và so sánh với baseline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Kết quả từ mô hình của chúng ta\n","our_results = {\n","    'Accuracy': test_results[0]['test_acc'],\n","    'F1 Score': test_results[0]['test_f1']\n","}\n","\n","# Kết quả từ bài báo\n","paper_results = {\n","    'Accuracy': 0.879,\n","    'F1 Score': 0.879\n","}\n","\n","# Tạo biểu đồ cột để so sánh kết quả\n","models = ['PhoBERT (Cài đặt của chúng ta)', 'MaxEnt (Bài báo gốc)']\n","metrics = ['Accuracy', 'F1 Score']\n","\n","x = np.arange(len(metrics))\n","width = 0.35\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","rects1 = ax.bar(x - width/2, [our_results[m] for m in metrics], width, label=models[0])\n","rects2 = ax.bar(x + width/2, [paper_results[m] for m in metrics], width, label=models[1])\n","\n","ax.set_ylabel('Điểm số')\n","ax.set_title('So sánh hiệu suất trên tập dữ liệu UIT-VSFC')\n","ax.set_xticks(x)\n","ax.set_xticklabels(metrics)\n","ax.legend()\n","\n","ax.bar_label(rects1, padding=3, fmt='%.3f')\n","ax.bar_label(rects2, padding=3, fmt='%.3f')\n","fig.tight_layout()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T05:46:19.142960Z","iopub.status.busy":"2025-03-29T05:46:19.142619Z","iopub.status.idle":"2025-03-29T05:46:19.149516Z","shell.execute_reply":"2025-03-29T05:46:19.148664Z","shell.execute_reply.started":"2025-03-29T05:46:19.142926Z"},"trusted":true},"outputs":[],"source":["print(\"Thảo luận:\")\n","print(\"Mô hình PhoBERT của chúng ta đạt độ chính xác {:.2f}% và điểm F1 {:.2f}%, \".format(\n","    our_results['Accuracy']*100, our_results['F1 Score']*100))\n","print(\"vượt trội hơn baseline MaxEnt từ bài báo gốc {:.2f}% \".format(\n","    (our_results['Accuracy'] - paper_results['Accuracy'])*100))\n","print(\"mà không cần điều chỉnh siêu tham số nhiều.\")\n","print(\"\\nCác cải tiến tiềm năng:\")\n","print(\"1. Lên lịch cho tốc độ học (learning rate scheduling)\")\n","print(\"2. Điều chỉnh siêu tham số rộng rãi hơn bằng cách sử dụng Wandb sweeps\")\n","print(\"3. Thử nghiệm các mô hình pre-trained khác (XLM-RoBERTa, BERTweet, v.v.)\")\n","print(\"4. Tăng cường dữ liệu cho các lớp không cân bằng\")\n","print(\"5. Phương pháp tổng hợp kết hợp nhiều mô hình\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-29T06:07:45.088782Z","iopub.status.busy":"2025-03-29T06:07:45.088461Z","iopub.status.idle":"2025-03-29T06:07:45.095213Z","shell.execute_reply":"2025-03-29T06:07:45.094391Z","shell.execute_reply.started":"2025-03-29T06:07:45.088759Z"},"trusted":true},"outputs":[],"source":["# In ra đường dẫn của checkpoint tốt nhất\n","print(f\"Đường dẫn đến checkpoint tốt nhất: {checkpoint_callback.best_model_path}\")\n","\n","# Nếu bạn muốn sao chép checkpoint tốt nhất vào một thư mục khác\n","import os\n","import shutil\n","\n","best_ckpt = checkpoint_callback.best_model_path\n","output_dir = '/kaggle/working/checkpoints'\n","os.makedirs(output_dir, exist_ok=True)\n","output_file = os.path.join(output_dir, os.path.basename(best_ckpt))\n","print(f\"Đã sao chép checkpoint tốt nhất vào: {output_file}\")"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-03-31T09:03:51.842852Z","iopub.status.busy":"2025-03-31T09:03:51.842545Z","iopub.status.idle":"2025-03-31T09:04:32.983329Z","shell.execute_reply":"2025-03-31T09:04:32.981582Z","shell.execute_reply.started":"2025-03-31T09:03:51.842812Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e867eee4ca24e99bb23d3c355e67dc0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36a63362c30c4782a1d5ae57463a58f5","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46a9d2343b1b4df0a557e30407abc9ba","version_major":2,"version_minor":0},"text/plain":["bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ed7534d694f4249b24829031abe6bc6","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OSError","evalue":"vinai/phobert-base does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bbdb62b77a01>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Tải model từ checkpoint đã lưu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vinai/phobert-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Loại bỏ prefix 'model.' nếu có\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3907\u001b[0m                             }\n\u001b[1;32m   3908\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mhas_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF2_WEIGHTS_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhas_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3909\u001b[0;31m                                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m   3910\u001b[0m                                     \u001b[0;34mf\"{pretrained_model_name_or_path} does not appear to have a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m                                     \u001b[0;34mf\" {_add_variant(WEIGHTS_NAME, variant)} but there is a file for TensorFlow weights.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: vinai/phobert-base does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."]}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import torch.nn.functional as F\n","\n","# Đường dẫn đến checkpoint tốt nhất\n","checkpoint_path = \"/kaggle/input/sentiment/phobert-sentiment-epoch08-val_f10.9431.ckpt\"\n","\n","# Tải tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","\n","# Tải model từ checkpoint đã lưu\n","model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n","state_dict = torch.load(checkpoint_path, map_location=torch.device('cuda'), weights_only=True)\n","# Loại bỏ prefix 'model.' nếu có\n","if all(k.startswith('model.') for k in state_dict['state_dict'].keys()):\n","    state_dict['state_dict'] = {k[6:]: v for k, v in state_dict['state_dict'].items() \n","                              if k.startswith('model.')}\n","model.load_state_dict(state_dict['state_dict'])\n","\n","# Đưa model về chế độ đánh giá\n","model.eval()\n","\n","# Mapping kết quả\n","id2label = {0: \"Tiêu cực\", 1: \"Trung tính\", 2: \"Tích cực\"}\n","\n","# Hàm dự đoán cảm xúc cho một câu\n","def predict_sentiment(text):\n","    # Tokenize\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n","    \n","    # Dự đoán\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        probabilities = F.softmax(logits, dim=-1)\n","        predicted_class = torch.argmax(probabilities, dim=1).item()\n","    \n","    # Lấy xác suất cho mỗi nhãn\n","    probs = probabilities[0].tolist()\n","    \n","    # Kết quả\n","    result = {\n","        \"text\": text,\n","        \"sentiment\": id2label[predicted_class],\n","        \"confidence\": probs[predicted_class],\n","        \"probabilities\": {id2label[i]: f\"{prob:.4f}\" for i, prob in enumerate(probs)}\n","    }\n","    \n","    return result\n","\n","# Test với một số câu ví dụ\n","test_sentences = [\n","    \"Khóa học này thật tuyệt vời, tôi học được rất nhiều điều.\",\n","    \"Bài giảng khá buồn tẻ và không hấp dẫn.\",\n","    \"Giáo viên giảng dạy rất nhiệt tình nhưng tài liệu hơi cũ.\",\n","    \"Tôi không thích cách tổ chức lớp học này.\",\n","    \"Phòng học sạch sẽ, thoáng mát và đầy đủ thiết bị.\",\n","    \"Chương trình học quá tải và khó hiểu.\",\n","    \"Bài tập về nhà quá nhiều và khó.\",\n","    \"Giáo viên trả lời câu hỏi rất chi tiết và dễ hiểu.\",\n","    \"Tôi thấy khóa học này không đáng tiền.\",\n","    \"Slide giáo trình đầy đủ và dễ hiểu.\"\n","]\n","\n","# Phân tích cảm xúc cho các câu thử nghiệm\n","for sentence in test_sentences:\n","    result = predict_sentiment(sentence)\n","    print(f\"Câu: {result['text']}\")\n","    print(f\"Cảm xúc: {result['sentiment']} (độ tin cậy: {result['confidence']:.4f})\")\n","    print(f\"Xác suất: {result['probabilities']}\")\n","    print(\"-\" * 80)\n","\n","# Tính độ chính xác trên tập dữ liệu có nhãn (nếu có)\n","def evaluate_accuracy(labeled_data):\n","    \"\"\"\n","    labeled_data là danh sách các tuple (text, label) với label là 0, 1, 2\n","    \"\"\"\n","    correct = 0\n","    total = len(labeled_data)\n","    \n","    for text, true_label in labeled_data:\n","        result = predict_sentiment(text)\n","        predicted_label = list(id2label.keys())[list(id2label.values()).index(result['sentiment'])]\n","        \n","        if predicted_label == true_label:\n","            correct += 1\n","            \n","    accuracy = correct / total\n","    print(f\"Độ chính xác trên {total} mẫu: {accuracy:.4f} ({correct}/{total})\")\n","    \n","    return accuracy\n","\n","# Nếu có tập dữ liệu đã gán nhãn sẵn, bạn có thể chạy:\n","# labeled_examples = [\n","#     (\"Giáo viên dạy rất hay\", 2),  # Tích cực\n","#     (\"Tôi không thích khóa học này\", 0),  # Tiêu cực\n","#     (\"Lớp học bình thường\", 1)  # Trung tính\n","# ]\n","# evaluate_accuracy(labeled_examples)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Câu châm biếm/mỉa mai (khó phân biệt tích cực/tiêu cực)\n","# Câu phủ định phức tạp\n","# Câu có cả yếu tố tích cực và tiêu cực"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6995399,"sourceId":11203773,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
